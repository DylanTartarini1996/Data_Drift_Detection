{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **DATA DRIFT DETECTION**\n",
    "Machine Learning models are unique and usually optimized for very specific problems, but their performance may nevertheless vary over time.  \n",
    "Team develops the model, tests it and everything works fine for some time. Then as time goes by, testing data is incorporated in the training set and new data come into production. One of the top reasons why performance of ML models decreases over time is caused by a drift in data.  \n",
    "Data drift is described as a change in the distribution of data over time.  \n",
    "This may occur in operational ML pipelines, when the drift occurs as the change in the distribution of the baseline data set used to train a particular model and the current, real-time production data. The distribution of the latter might be different because of any alteration in the real world: the **_stochastic process_** that generates the data in question has changed.  \n",
    "A classical example of this is the presence of seasonality in data, but drifts can also be caused by extreme events (pandemics, for example..); what is important to remember is that drifts happens when the statistical properties of the underlying variables that predict an outcome change.  \n",
    "Goal of this notebook is to explain two ways to run checks on your training and testing data to verify the presence of a data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of dataset\n",
    "First, we import a dataset for our experiment: I choose the **_Credit Card Fraud dataset_** freely available [kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud).  \n",
    "This dataset contains transactions made by credit cards in September 2013 by European cardholders, and is usually used for fraud detection and unbalanced classes studies: it presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.  \n",
    "The dataset contains only numerical input variables which are the result of a **_PCA transformation_**. Due to confidentiality issues, original authors cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.  \n",
    "Now this dataset is very handy for our example because since the features come from PCA performed on real data, we do not (and can not) know anything about them. But we can still verify the presence of a drift whenever new data comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform some very basic exploration of the dataset in our possession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      float64\n",
       "V1        float64\n",
       "V2        float64\n",
       "V3        float64\n",
       "V4        float64\n",
       "V5        float64\n",
       "V6        float64\n",
       "V7        float64\n",
       "V8        float64\n",
       "V9        float64\n",
       "V10       float64\n",
       "V11       float64\n",
       "V12       float64\n",
       "V13       float64\n",
       "V14       float64\n",
       "V15       float64\n",
       "V16       float64\n",
       "V17       float64\n",
       "V18       float64\n",
       "V19       float64\n",
       "V20       float64\n",
       "V21       float64\n",
       "V22       float64\n",
       "V23       float64\n",
       "V24       float64\n",
       "V25       float64\n",
       "V26       float64\n",
       "V27       float64\n",
       "V28       float64\n",
       "Amount    float64\n",
       "Class       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Time', axis=1) # We are not interested in the variable 'Time' for this excercise, so just drop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of frauds in this dataset? That is to say, what is how many frauds out of all the transactions in the dset are there?  \n",
    "Remember that dset contains information for only two days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of frauds in the dataset is  0.173 % of total transactions\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of frauds in the dataset is ', np.round((np.sum(df.Class) / len(df))*100, 3), '% of total transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original training and testing set ##  \n",
    "Since this is a working example, we perform the original train-test splitting ourselves. Remember that ideally, testing in production should be **_newly acquired data_** that our model hasn't see previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199364, 29) (85443, 29) (199364,) (85443,)\n"
     ]
    }
   ],
   "source": [
    "X_orig, y_orig = df.loc[:, df.columns != 'Class'], df['Class']\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, random_state=1, stratify=y_orig) \n",
    "print(X_train_orig.shape, X_test_orig.shape, y_train_orig.shape, y_test_orig.shape) # making sure that train-test splitting happens with the same fraud pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of frauds in the train dset is  0.173 % of total transactions\n",
      "AND\n",
      "Percentage of frauds in the train dset is  0.173 % of total transactions\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of frauds in the train dset is ', np.round((np.sum(y_train_orig) / len(y_train_orig))*100, 3), '% of total transactions') #check\n",
    "print('AND')\n",
    "print('Percentage of frauds in the train dset is ', np.round((np.sum(y_test_orig) / len(y_test_orig))*100, 3), '% of total transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Drift detection with **Random Forest** ##\n",
    "We want to know wheter or not there's a drift between training and testing phase.  \n",
    "In production, every now and then (may be on daily, weekly, monthly basis..) new data comes in: we want to discover if new data generates from the same stochastic process that generated \"old\" data (which is, of course, still in the training set!).  \n",
    "How to do this? The answer is quite simple.  \n",
    "We translate the question this way: is there a way a (simple) Machine Learning algorythm can distinguish between training and testing set in a deterministic way? If the answer is yes, then we may be having a data drift problem.  \n",
    "Let's start by labelling with zeros observations from the original training set, while the original testing data will be labelled with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig['Target'] = 0 \n",
    "X_test_orig['Target'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then join training and testing set and proceed to shuffle the new dataframe.  \n",
    "The real Class value (fraud/not a fraud) is none of our business, because we really just want to check if the stochastic process that generates frauds is still the same between training and testing, therefore we do not care about the 'Class' feature in the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.574501</td>\n",
       "      <td>1.117222</td>\n",
       "      <td>1.635889</td>\n",
       "      <td>2.855814</td>\n",
       "      <td>-0.369436</td>\n",
       "      <td>-0.180819</td>\n",
       "      <td>0.209807</td>\n",
       "      <td>0.228980</td>\n",
       "      <td>-0.703786</td>\n",
       "      <td>0.527349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047201</td>\n",
       "      <td>0.329075</td>\n",
       "      <td>-0.004759</td>\n",
       "      <td>0.718633</td>\n",
       "      <td>-0.267540</td>\n",
       "      <td>0.079824</td>\n",
       "      <td>-0.315484</td>\n",
       "      <td>-0.224792</td>\n",
       "      <td>12.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.297272</td>\n",
       "      <td>-3.596179</td>\n",
       "      <td>-3.491096</td>\n",
       "      <td>0.429110</td>\n",
       "      <td>-0.885160</td>\n",
       "      <td>-1.408148</td>\n",
       "      <td>1.884916</td>\n",
       "      <td>-0.914348</td>\n",
       "      <td>-1.192692</td>\n",
       "      <td>0.580871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506746</td>\n",
       "      <td>-0.204007</td>\n",
       "      <td>-0.966447</td>\n",
       "      <td>0.097492</td>\n",
       "      <td>0.099141</td>\n",
       "      <td>0.916954</td>\n",
       "      <td>-0.312858</td>\n",
       "      <td>0.081973</td>\n",
       "      <td>1018.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.708808</td>\n",
       "      <td>0.777698</td>\n",
       "      <td>1.059110</td>\n",
       "      <td>-0.455618</td>\n",
       "      <td>0.345712</td>\n",
       "      <td>-0.893250</td>\n",
       "      <td>1.396244</td>\n",
       "      <td>-0.381813</td>\n",
       "      <td>-0.329686</td>\n",
       "      <td>-0.172026</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403242</td>\n",
       "      <td>-0.748797</td>\n",
       "      <td>0.336336</td>\n",
       "      <td>0.383539</td>\n",
       "      <td>-0.370202</td>\n",
       "      <td>-0.292606</td>\n",
       "      <td>-0.754060</td>\n",
       "      <td>-0.463178</td>\n",
       "      <td>69.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.047911</td>\n",
       "      <td>0.264798</td>\n",
       "      <td>2.882793</td>\n",
       "      <td>1.050493</td>\n",
       "      <td>-1.754437</td>\n",
       "      <td>1.013074</td>\n",
       "      <td>-1.216518</td>\n",
       "      <td>0.993989</td>\n",
       "      <td>-0.289283</td>\n",
       "      <td>0.066573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.392708</td>\n",
       "      <td>-0.197803</td>\n",
       "      <td>-0.109727</td>\n",
       "      <td>0.396181</td>\n",
       "      <td>0.181980</td>\n",
       "      <td>-0.065889</td>\n",
       "      <td>0.328321</td>\n",
       "      <td>0.089068</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.249711</td>\n",
       "      <td>-0.694850</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.444425</td>\n",
       "      <td>-0.810558</td>\n",
       "      <td>-0.589118</td>\n",
       "      <td>-0.246620</td>\n",
       "      <td>-0.154379</td>\n",
       "      <td>-0.756298</td>\n",
       "      <td>0.562567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390296</td>\n",
       "      <td>-0.630127</td>\n",
       "      <td>-0.022713</td>\n",
       "      <td>0.114441</td>\n",
       "      <td>0.327275</td>\n",
       "      <td>1.116745</td>\n",
       "      <td>-0.065491</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>64.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.574501  1.117222  1.635889  2.855814 -0.369436 -0.180819  0.209807   \n",
       "1  0.297272 -3.596179 -3.491096  0.429110 -0.885160 -1.408148  1.884916   \n",
       "2 -1.708808  0.777698  1.059110 -0.455618  0.345712 -0.893250  1.396244   \n",
       "3 -1.047911  0.264798  2.882793  1.050493 -1.754437  1.013074 -1.216518   \n",
       "4  1.249711 -0.694850  0.000151 -0.444425 -0.810558 -0.589118 -0.246620   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.228980 -0.703786  0.527349  ...  0.047201  0.329075 -0.004759  0.718633   \n",
       "1 -0.914348 -1.192692  0.580871  ...  0.506746 -0.204007 -0.966447  0.097492   \n",
       "2 -0.381813 -0.329686 -0.172026  ... -0.403242 -0.748797  0.336336  0.383539   \n",
       "3  0.993989 -0.289283  0.066573  ... -0.392708 -0.197803 -0.109727  0.396181   \n",
       "4 -0.154379 -0.756298  0.562567  ... -0.390296 -0.630127 -0.022713  0.114441   \n",
       "\n",
       "        V25       V26       V27       V28   Amount  Target  \n",
       "0 -0.267540  0.079824 -0.315484 -0.224792    12.52       1  \n",
       "1  0.099141  0.916954 -0.312858  0.081973  1018.00       0  \n",
       "2 -0.370202 -0.292606 -0.754060 -0.463178    69.90       0  \n",
       "3  0.181980 -0.065889  0.328321  0.089068    12.00       0  \n",
       "4  0.327275  1.116745 -0.065491  0.003907    64.99       0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [X_train_orig, X_test_orig]   \n",
    "drifting_df = pd.concat(frames)\n",
    "drifting_df = drifting_df.sample(frac=1).reset_index(drop=True)   # shuffling the new df\n",
    "drifting_df.shape\n",
    "drifting_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to train a Random Forest Model on the newly obtained dataset, keeping in mind that our target is now the 'Target' variable, that tells us whether or not the observation comes from the **_original_** training and testing set.  \n",
    "Clearly, we start from random splitting.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199364, 29) (85443, 29) (199364,) (85443,)\n"
     ]
    }
   ],
   "source": [
    "X, y = drifting_df.loc[:, drifting_df.columns != 'Target'], drifting_df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's stop and think about what we want this Random Forest Classifier to do.  \n",
    "Theoretically, if there is no drift in our data, if there is no deterministic process that generates data, RF Classifier shouldn't do better than the toss of a coin, which means that _Accuracy should be around 0.5_.  \n",
    "If that isn't the case, we should be worried, because something in the stochastic process from which we have drawn the original training and testing set is not stochastic at all: the process changed inbetween the moment we collected the original training set and the moment new data (the original testing set) came into our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6999988296291094"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl.metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be a problem: how can the RF classifier have an accuracy score of nearly 70% if the split is random and it has not been told whether an observation was in the original training or testing set?  \n",
    "One way to answer this question might be to gather **_feature importances_**: usually, this is used to explain to an audience what are the most informative features in the decision trees that build the Random Forest ensemble, but here we use this method to understand what might be the features whose generative process changed the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "forest_importances = pd.Series(importances, index = X_train.columns)\n",
    "forest_importances.sort_values(axis=0, ascending=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances in data drift detection\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know what might be the features that changed the most their behaviour between training and testing period: of course, it will be the decision of the Data Scientist if and when to investigate further on the data drift theme. Said decision should be based on domain knowledge of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Kolmogorov-Smirnov Test_**\n",
    "A very efficient way to determine whether two samples are significantly different from one another is the Kolmogorov-Smirnov test: KS statistic quantifies distance between the empirical distribution functions of two samples, and it does so detecting in-sample variance. This is particularly useful in our case, because we are interested in knowing if data in the training and testing set originates from different stochastic processes, that is to say if we are looking into a data drift problem. Of course, we want to check this for **EVERY feature** in our data.  \n",
    "Now, there is a function in the _stats_ package, called **_ks_2samp_**, that does exactly that, and takes two _np.arrays_ (in our case, a pair of the same feature from training and testing set) as an argument. This might be a problem, because our dataset could have tenths, hundres of features. We need to write a couple functions to avoid doing this manually. That can easily be done with a combination of data wrangling in _pandas_ and a _for loop_.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning ##\n",
    "Please note that KS test only works on variables which distribution is **_continuos_**. We only have one non-continuos feature in our dataset, so we can just as well ignore it, but if you have multiple continuos features in your data might actually be a good idea to rewrite the for loop to run only for continuos variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_test_dd(feat1:pd.Series, feat2:pd.Series):\n",
    "# performs Kolmogorov-Smirnov two-sided sample test on 2 features from different datasets #\n",
    "# H0: two samples are drawn from the exact same distribution (the lower p-val, the higher chance distributions are different) #\n",
    "# RETURNS: a df with stat (float) and p-value (float) #\n",
    "\n",
    "    f1 = np.array(feat1.sample(n=10000))  #sampling the features and transforming them into arrays\n",
    "    f2 = np.array(feat2.sample(n=10000))\n",
    "\n",
    "    res = pd.DataFrame(sp.stats.ks_2samp(f1, f2, alternative=\"two-sided\", mode=\"auto\")) #running KS test on training and testing set features\n",
    "    res = res.T\n",
    "    res.columns = ['stat', 'p-value']\n",
    "\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_drift_ks(train:pd.DataFrame, test:pd.DataFrame):\n",
    "# performs Kolmogorov-Smirnov two-sided sample test on each pair of features that are present both in training and testing data #\n",
    "# H0: two samples are drawn from the exact same distribution (the lower p-val, the higher chance distributions are different) #\n",
    "# RETURNS: a df with three columns: stat (float), p-value (float) and reject_H0 (bool) #\n",
    "\n",
    "    feats = train.columns\n",
    "    ks_res = pd.DataFrame(columns=['stat', 'p-value'])\n",
    "\n",
    "    for v in feats:\n",
    "        test_res = ks_test_dd(X_train_orig[v], X_test_orig[v])\n",
    "        test_res.index = [str(v)]\n",
    "        frames = [ks_res, test_res]\n",
    "        ks_res = pd.concat(frames)\n",
    "\n",
    "    ks_res['reject_H0'] = ks_res['p-value'] < 0.05 \n",
    "\n",
    "    return ks_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stat</th>\n",
       "      <th>p-value</th>\n",
       "      <th>reject_H0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.687549</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.446296</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.534270</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.843589</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.823267</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.122950</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.118902</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.906228</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.258234</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.746166</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.722940</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.872190</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.258234</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.127111</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.039664</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.823267</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.236906</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.967082</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V19</th>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.833544</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V20</th>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.823267</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V21</th>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.385748</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V22</th>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.198255</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V23</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.580642</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V24</th>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.415380</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V25</th>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.236906</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V26</th>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.204335</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V27</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.580642</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V28</th>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.853388</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount</th>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.366736</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          stat   p-value  reject_H0\n",
       "V1      0.0101  0.687549      False\n",
       "V2      0.0122  0.446296      False\n",
       "V3      0.0114  0.534270      False\n",
       "V4      0.0087  0.843589      False\n",
       "V5      0.0089  0.823267      False\n",
       "V6      0.0167  0.122950      False\n",
       "V7      0.0168  0.118902      False\n",
       "V8      0.0080  0.906228      False\n",
       "V9      0.0143  0.258234      False\n",
       "V10     0.0096  0.746166      False\n",
       "V11     0.0098  0.722940      False\n",
       "V12     0.0084  0.872190      False\n",
       "V13     0.0143  0.258234      False\n",
       "V14     0.0166  0.127111      False\n",
       "V15     0.0198  0.039664       True\n",
       "V16     0.0089  0.823267      False\n",
       "V17     0.0146  0.236906      False\n",
       "V18     0.0070  0.967082      False\n",
       "V19     0.0088  0.833544      False\n",
       "V20     0.0089  0.823267      False\n",
       "V21     0.0128  0.385748      False\n",
       "V22     0.0152  0.198255      False\n",
       "V23     0.0110  0.580642      False\n",
       "V24     0.0125  0.415380      False\n",
       "V25     0.0146  0.236906      False\n",
       "V26     0.0151  0.204335      False\n",
       "V27     0.0110  0.580642      False\n",
       "V28     0.0086  0.853388      False\n",
       "Amount  0.0130  0.366736      False\n",
       "Target  1.0000  0.000000       True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_drift_ks(X_train_orig, X_test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! As we can see from the output table, we don't reject the null hyphotesis for any feature in the dataset: remember that 'Target' was added by us and marks observation belonging to the original training or testing set: it does not come from a continuos distribution, so we just ignore that result. This brings us to assert that for each feature, there is no significant difference in its distribution across training and testing set, which is of course what we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "We have briefly explored Data Drift theme and two ways to detect it in advance. The suggestion is to implement one or both checks, RF and KS test, in your ML pipeline, right after importing new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements ##\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (UniversitÃ© Libre de Bruxelles) on big data mining and fraud detection.\n",
    "More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
    "\n",
    "Please cite the following works:\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-AÃ«l Le Borgne, Liyun He, Frederic OblÃ©, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-AÃ«l Le Borgne, Olivier Caelen, Frederic OblÃ©, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n",
    "\n",
    "Yann-AÃ«l Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f14ba2ee5a049ba211b8d4d2cb272f932c8c352687f45f1fa6cbec4436d95677"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
